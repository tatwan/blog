<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Linear Separability | TechTalks</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Linear Separability" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Methods for Testing Linear Separability in Python" />
<meta property="og:description" content="Methods for Testing Linear Separability in Python" />
<link rel="canonical" href="http://tarekatwan.com/blog/markdown/2017/12/31/linear-separability.html" />
<meta property="og:url" content="http://tarekatwan.com/blog/markdown/2017/12/31/linear-separability.html" />
<meta property="og:site_name" content="TechTalks" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-12-31T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Methods for Testing Linear Separability in Python","@type":"BlogPosting","headline":"Linear Separability","dateModified":"2017-12-31T00:00:00-06:00","datePublished":"2017-12-31T00:00:00-06:00","url":"http://tarekatwan.com/blog/markdown/2017/12/31/linear-separability.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://tarekatwan.com/blog/markdown/2017/12/31/linear-separability.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://tarekatwan.com/blog/feed.xml" title="TechTalks" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Linear Separability | TechTalks</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Linear Separability" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Methods for Testing Linear Separability in Python" />
<meta property="og:description" content="Methods for Testing Linear Separability in Python" />
<link rel="canonical" href="http://tarekatwan.com/blog/markdown/2017/12/31/linear-separability.html" />
<meta property="og:url" content="http://tarekatwan.com/blog/markdown/2017/12/31/linear-separability.html" />
<meta property="og:site_name" content="TechTalks" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-12-31T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Methods for Testing Linear Separability in Python","@type":"BlogPosting","headline":"Linear Separability","dateModified":"2017-12-31T00:00:00-06:00","datePublished":"2017-12-31T00:00:00-06:00","url":"http://tarekatwan.com/blog/markdown/2017/12/31/linear-separability.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://tarekatwan.com/blog/markdown/2017/12/31/linear-separability.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="http://tarekatwan.com/blog/feed.xml" title="TechTalks" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">TechTalks</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Linear Separability</h1><p class="page-description">Methods for Testing Linear Separability in Python</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2017-12-31T00:00:00-06:00" itemprop="datePublished">
        Dec 31, 2017
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      15 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#linear-vs-non-linear-classification">Linear vs Non-Linear Classification</a>
<ul>
<li class="toc-entry toc-h2"><a href="#domain-knowledgeexpertise">Domain Knowledge/Expertise</a></li>
<li class="toc-entry toc-h2"><a href="#getting-our-data">Getting our data</a></li>
<li class="toc-entry toc-h2"><a href="#data-visiualization">Data Visiualization</a></li>
<li class="toc-entry toc-h2"><a href="#computational-geometry">Computational Geometry</a></li>
<li class="toc-entry toc-h2"><a href="#linear-programming">Linear Programming</a></li>
<li class="toc-entry toc-h2"><a href="#machine-learning">Machine Learning</a>
<ul>
<li class="toc-entry toc-h3"><a href="#single-layer-perceptron">Single Layer Perceptron</a></li>
<li class="toc-entry toc-h3"><a href="#support-vector-machines">Support Vector Machines</a></li>
</ul>
</li>
</ul>
</li>
</ul><h1 id="linear-vs-non-linear-classification">
<a class="anchor" href="#linear-vs-non-linear-classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear vs Non-Linear Classification</h1>

<p>Two subsets are said to be linearly separable if there exists a hyperplane that separates the elements of each set in a way that all elements of one set resides on the opposite side of the hyperplane from the other set. In 2D plotting, we can depict this through a separation line, and in 3D plotting through a hyperplane.</p>

<p>By definition Linear Separability is defined:</p>

<p>Two sets $H = { H^1,\cdots,H^h } \subseteq \mathbb{R}^d$ and $M = { M^1,\cdots,M^m } \subseteq \mathbb{R}^d$ are said to be linearly separable if $\exists a \in \mathbb{R}^n$, $b \in \mathbb{R} : H \subseteq { x \in \mathbb{R}^n : a^T x &gt; b }$ and $M \subseteq { x \in \mathbb{R}^n : a^Tx \leq b }$ <a href="http://www.tarekatwan.com/wp-admin/post.php?post=102&amp;action=edit#fn-102-1">1</a></p>

<p>In simple words, the expression above states that H and M are linearly separable if there exists a hyperplane that completely separates the elements of [latex]H [/latex] and elements of $M$.</p>

<p><img src="http://www.tarekatwan.com/wp-content/uploads/2017/12/linear_sep-1024x419.png" alt="" width="1000" height="409" class="alignnone size-large wp-image-202">
<em>Image source from Sebastian Raschka</em> <sup id="fnref:2"><a href="#fn:2" class="footnote">1</a></sup></p>

<p>In the figure above, (A) shows a linear classification problem and (B) shows a non-linear classification problem. In (A) our decision boundary is a linear one that completely separates the blue dots from the green dots. In this scenario several linear classifiers can be implemented.</p>

<p>In (B) our decision boundary is non-linear and we would be using non-linear kernel functions and other non-linear classification algorithms and techniques.</p>

<p>Generally speaking, in Machine Learning and before running any type of classifier, it is important to understand the data we are dealing with to determine which algorithm to start with, and which parameters we need to adjust that are suitable for the task. This brings us to the topic of linear separability and understanding if our problem is linear or non-linear.</p>

<p>As states above, there are several classification algorithms that are designed to separate the data by constructing a linear decision boundary (hyperplane) to divide the classes and with that comes the assumption: that the data is linearly separable. Now, in real world scenarios things are not that easy and data in many cases may not be linearly separable and thus non-linear techniques are applied. Without digging too deep, the decision of linear vs non-linear techniques is a decision the data scientist need to make based on what they know in terms of the end goal, what they are willing to accept in terms of error, the balance between model complexity and generalization, bias-variance tradeoff ..etc.</p>

<p>This post was inspired by research papers on the topic of linear separability including <u>The Linear Separability Problem: Some Testing Method</u> <sup id="fnref:3"><a href="#fn:3" class="footnote">2</a></sup>, <sup id="fnref:4"><a href="#fn:4" class="footnote">3</a></sup></p>

<p>My goal in this post is to apply and test few techniques in python and demonstrate how they can be implemented. Some of those techniques for testing linear separability are:</p>

<ul>
  <li><a href="#1">Domain Knowledge and Expertise</a></li>
  <li><a href="#2">Data Visualization</a></li>
  <li><a href="#3">Computational Geometry (Convex Hulls)</a></li>
  <li><a href="#4">Linear Programming</a></li>
  <li>
<a href="#5">Machine Learning:</a>
    * <a href="#6">Perceptron</a>
    * <a href="#7">Support Vector Machine</a>
</li>
</ul>

<h2 id="domain-knowledgeexpertise">
<a class="anchor" href="#domain-knowledgeexpertise" aria-hidden="true"><span class="octicon octicon-link"></span></a>Domain Knowledge/Expertise</h2>

<p>It should be a no-brainer that the first step should always be to seek insight from analysts and other data scientists who are already dealing with the data and familiar with it. It is critical before embarking on any data discovery journey to always start by asking questions to better understand the purpose of the task (your goal) and gain early insight into the data from the domain experts (business data users , data/business analysts or data scientists) that are closer to the data and deal with it daily.</p>

<h2 id="getting-our-data">
<a class="anchor" href="#getting-our-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Getting our data</h2>

<p>For the other four (4) approaches listed above, we will explore these concepts using the classic <a href="https://archive.ics.uci.edu/ml/datasets/iris">Iris data set</a> and implement some of the theories behind testing for linear separability using Python.</p>

<p>Since this is a well known data set we know in advance which classes are linearly separable (domain knowledge/past experiences coming into play here).For our analysis we will use this knowledge to confirm our findings.</p>

<blockquote>
  <p>The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.</p>
</blockquote>

<p>Let’s get things ready first by importing the necessary libraries and loading our data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>

<span class="c1">#create a DataFrame
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s">'Target'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">sepal Length (cm)</th>
      <th style="text-align: center">sepal width (cm)</th>
      <th style="text-align: center">petal length (cm)</th>
      <th style="text-align: center">petal width (cm)</th>
      <th style="text-align: center">Target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: center">5.1</td>
      <td style="text-align: center">3.5</td>
      <td style="text-align: center">1.4</td>
      <td style="text-align: center">0.2</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">4.9</td>
      <td style="text-align: center">3.0</td>
      <td style="text-align: center">1.4</td>
      <td style="text-align: center">0.2</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">4.7</td>
      <td style="text-align: center">3.2</td>
      <td style="text-align: center">1.3</td>
      <td style="text-align: center">0.2</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">4.6</td>
      <td style="text-align: center">3.1</td>
      <td style="text-align: center">1.5</td>
      <td style="text-align: center">0.2</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">5.0</td>
      <td style="text-align: center">3.6</td>
      <td style="text-align: center">1.4</td>
      <td style="text-align: center">0.2</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<h2 id="data-visiualization">
<a class="anchor" href="#data-visiualization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Visiualization</h2>

<p>The simplest and quickest method is to visualize the data. This approach may not be feasible or as straight forward if the number of features is large, making it hard to plot in 2D . In such a case, we can use a Pair Plot approach, and pandas gives us a great option to do so with <code class="highlighter-rouge">scatter_matrix</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pandas.tools.plotting</span> <span class="kn">import</span> <span class="n">scatter_matrix</span>
<span class="n">scatter_matrix</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">11</span><span class="p">))</span>

</code></pre></div></div>

<p><img class="alignnone size-full wp-image-86" src="http://www.tarekatwan.com/wp-content/uploads/2017/12/fig1.png" alt="" width="888" height="649"></p>

<p>The scatter matrix above is a pair-wise scatter plot for all features in the data set (we have four features so we get a 4x4 matrix). The scatter matrix provides insight into how these variables are correlated. Let’s expand upon this by creating a scatter plot for the Petal Length vs Petal Width from the scatter matrix.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Petal Width vs Petal Length'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="alignnone size-full wp-image-87" src="http://www.tarekatwan.com/wp-content/uploads/2017/12/fig2.png" alt="" width="612" height="387"></p>

<p>It’s still not that helpful. Let’s color each class and add a legend so we can understand what the plot is trying to convey in terms of data distribution per class and determine if the classes can be linearly separable visually.</p>

<p>Let’s update our code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">names</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">target_names</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s">'b'</span><span class="p">,</span><span class="s">'r'</span><span class="p">,</span><span class="s">'g'</span><span class="p">]</span>
<span class="n">label</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">int</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Petal Width vs Petal Length'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">)):</span>
    <span class="n">bucket</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">'Target'</span><span class="p">]</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span>
    <span class="n">bucket</span> <span class="o">=</span> <span class="n">bucket</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">bucket</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bucket</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">names</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="alignnone size-full wp-image-88" src="http://www.tarekatwan.com/wp-content/uploads/2017/12/fig3.png" alt="" width="612" height="387"></p>

<p>Much better. We just plotted the entire data set, all 150 points. There are 50 data points per class. And Yes, at first glance we can see that the blue dots (Setosa class) can be easily separated by drawing a line and segregate it from the rest of the classes. But what about the other two classes?</p>

<p>Let’s examine another approach to be more certain.</p>

<h2 id="computational-geometry">
<a class="anchor" href="#computational-geometry" aria-hidden="true"><span class="octicon octicon-link"></span></a>Computational Geometry</h2>

<p>In this approach we will use a <strong><a href="https://en.wikipedia.org/wiki/Convex_hull">Convex Hull</a></strong> to check whether a particular class is linearly separable or not from the rest. In simplest terms, the convex hull represents the outer boundaries of a group of data points (classes) which is why sometimes it’s called the convex envelope.</p>

<p>The logic when using convex hulls when testing for linear separability is pretty straight forward which can be stated as:</p>

<blockquote>
  <p>Two classes X and Y are LS (Linearly Separable) if the intersection of the convex hulls of X and Y is empty, and NLS (Not Linearly Separable) with a non-empty intersection.</p>
</blockquote>

<p>A quick way to see how this works is to visualize the data points with the convex hulls for each class. We will plot the hull boundaries to examine the intersections visually. We will be using the <strong>Scipy</strong> library to help us compute the convex hull. For more information please refer to <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.ConvexHull.html">Scipy documentation</a>.</p>

<p>Let’s update the previous code to include convex hulls.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.spatial</span> <span class="kn">import</span> <span class="n">ConvexHull</span>

<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">names</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">target_names</span>
<span class="n">label</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">int</span><span class="p">)</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s">'b'</span><span class="p">,</span><span class="s">'r'</span><span class="p">,</span><span class="s">'g'</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Petal Width vs Petal Length'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">)):</span>
    <span class="n">bucket</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">'Target'</span><span class="p">]</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span>
    <span class="n">bucket</span> <span class="o">=</span> <span class="n">bucket</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
    <span class="n">hull</span> <span class="o">=</span> <span class="n">ConvexHull</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">bucket</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bucket</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">names</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">hull</span><span class="o">.</span><span class="n">simplices</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bucket</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">bucket</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>And our output should look like this:</p>

<p><img class="alignnone size-full wp-image-89" src="http://www.tarekatwan.com/wp-content/uploads/2017/12/fig4.png" alt="" width="612" height="387"></p>

<p>It is more obvious now, visually at least, that Setosa is a linearly separable class form the other two. In other words, we can easily draw a straight line to separate Setosa from non-Setosa (Setosas vs. everything else). Both Versicolor and Virginica classes are not linearly separable because we can see there is indeed an intersection.</p>

<h2 id="linear-programming">
<a class="anchor" href="#linear-programming" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear Programming</h2>

<p>By definition Linear Separability is defined:</p>

<p>Two sets $H = { H^1,\cdots,H^h } \subseteq \mathbb{R}^d$ and $M = { M^1,\cdots,M^m } \subseteq \mathbb{R}^d$ are said to be linearly separable if $\exists a \in \mathbb{R}^n$, $b \in \mathbb{R} : H \subseteq { x \in \mathbb{R}^n : a^T x \gt; b }$  and $M \subseteq { x \in \mathbb{R}^n : a^Tx \leq b }$ <sup id="fnref:4:1"><a href="#fn:4" class="footnote">3</a></sup></p>

<p>In simple words, the expression above states that H and M are linearly separable if there exists a hyperplane that completely separates the elements of $H$ and elements of $M$.</p>

<p>$H$ and $M$ are linearly separable if the optimal value of Linear Program $(LP)$ is $0$</p>

<p>Here is a great post that implements this in R which I followed as an inspiration for this section on linear programming with python: <a href="https://www.joyofdata.de/blog/testing-linear-separability-linear-programming-r-glpk/">Testing for Linear Separability with LP in R</a> <sup id="fnref:5"><a href="#fn:5" class="footnote">4</a></sup>.</p>

<p>Below is the code in python using scipy <code class="highlighter-rouge">linprog(method='simplex')</code> to solve our linear programming problem. If we examine the output, using LP (Linear Programming) method we can conclude that it is possible to have a hyperplane that linearly separates Setosa from the rest of the classes, which is the only linearly separable class from the rest.</p>

<p>If the problem is solvable, the Scipy output will provide us with additional information:</p>

<table>
  <thead>
    <tr>
      <th>Returns</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
<strong>success</strong>: bool</td>
      <td>True or False (True if a solution was found)</td>
    </tr>
    <tr>
      <td>
<strong>status</strong>: int</td>
      <td>
<strong>0</strong> : Optimization terminated successfully, <strong>1</strong> : Iteration limit reached, <strong>2</strong> : Problem appears to be infeasible, <strong>3</strong> : Problem appears to be unbounded</td>
    </tr>
    <tr>
      <td>
<strong>message</strong> : str</td>
      <td>Describing the status</td>
    </tr>
    <tr>
      <td>
<strong>x</strong>: ndarray</td>
      <td>The independent variable vector which optimizes the linear programming problem.</td>
    </tr>
    <tr>
      <td>
<strong>slack</strong>: ndarray</td>
      <td>The values of the slack variables. Each slack variable corresponds to an inequality constraint. If the slack is zero, then the corresponding constraint is active.</td>
    </tr>
    <tr>
      <td>
<strong>nit</strong> : int</td>
      <td>The number of iterations performed.</td>
    </tr>
    <tr>
      <td>
<strong>fun</strong> : float</td>
      <td>Value of the objective function</td>
    </tr>
  </tbody>
</table>

<p>For our example, I am only looking at the status/success to determine if a solution was found or not.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">linprog</span>
 <span class="n">dic</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s">'setosa'</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s">'versicolor'</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s">'verginica'</span><span class="p">}</span>
 
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dic</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="n">df</span><span class="p">[</span><span class="s">"newTarget"</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'Target'</span><span class="p">]</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span> <span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
     
    <span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
 
    <span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">newTarget</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">tmp</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'Target'</span><span class="p">]</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span> <span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
     
    <span class="c1">#2-D array which, when matrix-multiplied by x, gives the values of 
</span>    <span class="c1">#the upper-bound inequality constraints at x.
</span>    <span class="n">A_ub</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
     
    <span class="c1">#1-D array of values representing the upper-bound of each 
</span>    <span class="c1">#inequality constraint (row) in A_ub.
</span>    <span class="n">b_ub</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">A_ub</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
     
    <span class="c1"># Coefficients of the linear objective function to be minimized.
</span>    <span class="n">c_obj</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">A_ub</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">linprog</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="n">c_obj</span><span class="p">,</span> <span class="n">A_ub</span><span class="o">=</span><span class="n">A_ub</span><span class="p">,</span> <span class="n">b_ub</span><span class="o">=</span><span class="n">b_ub</span><span class="p">,</span>
                  <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s">"disp"</span><span class="p">:</span> <span class="bp">False</span><span class="p">})</span>
     
    <span class="k">if</span> <span class="n">res</span><span class="o">.</span><span class="n">success</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'There is linear separability between {} and the rest'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">dic</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'No linear separability between {} and the rest'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">dic</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt;&gt;output
There is linear separability between setosa and the rest
No linear separability between versicolor and the rest
No linear separability between verginica and the rest
</code></pre></div></div>

<h2 id="machine-learning">
<a class="anchor" href="#machine-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Machine Learning</h2>

<p>In this section we will examine two classifiers for the purpose of testing for linear separability: the <strong><a href="https://en.wikipedia.org/wiki/Perceptron">Perceptron</a></strong> (simplest form of Neural Networks) and <strong><a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support Vector Machines</a></strong> (part of a class known as <a href="https://en.wikipedia.org/wiki/Kernel_method">Kernel Methods</a>)</p>

<h3 id="single-layer-perceptron">
<a class="anchor" href="#single-layer-perceptron" aria-hidden="true"><span class="octicon octicon-link"></span></a>Single Layer Perceptron</h3>

<p>The <strong>perceptron</strong> is an algorithm used for binary classification and belongs to a class of linear classifiers. In binary classification, we are trying to separate data into two buckets: either you are in Buket A or Bucket B. This can be stated even simpler: either you are in Bucket A or not in Bucket A (assuming we have only two classes) and hence the name binary classification.
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathsize="1.2em"><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mstyle scriptlevel="0" displaystyle="true"><mn>1</mn></mstyle></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if w . x + b</mtext><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>otherwise</mtext></mstyle></mtd></mtr></mtable></mrow></mstyle></mrow><annotation encoding="application/x-tex">\large \begin{cases} \displaystyle 1 &amp;\text {if w . x + b} &gt; {0}\\ 0 &amp;\text {otherwise} \end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.600040000000001em;vertical-align:-1.50002em;"></span><span class="minner sizing reset-size6 size7"><span class="mopen sizing reset-size7 size6"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.1000200000000007em;"><span style="top:-2.54999em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎩</span></span></span><span style="top:-2.2549900000000003em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎪</span></span></span><span style="top:-3.2000100000000002em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎨</span></span></span><span style="top:-4.05501em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎪</span></span></span><span style="top:-4.35002em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎧</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.50002em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.882em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord"><span class="mord">1</span></span></span><span style="top:-2.442em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord"><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.882em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord"><span class="mord text"><span class="mord">if w . x + b</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord">0</span></span></span></span><span style="top:-2.442em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord"><span class="mord text"><span class="mord">otherwise</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size7 size6"></span></span></span></span></span></p>

<p>A single layer perceptron will only converge if the input vectors are linearly separable. In this state, all input vectors would be classified correctly indicating linear separability. It will not converge if they are not linearly separable. In other words, it will not classify correctly if the data set is not linearly separable. For our testing purpose, this is exactly what we need.</p>

<p>We will apply it on the entire data instead of splitting to test/train since our intent is to test for linear separability among the classes and not to build a model for future predictions.</p>

<p>We will use Scikit-Learn and pick the Perceptron as our linear model selection. Before that, let’s do some basic data preprocessing tasks:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Data Preprocessing
</span><span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="c1"># we are picking Setosa to be 1 and all other classes will be 0
</span><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">int</span><span class="p">)</span> 

<span class="c1">#Perform feature scaling
</span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">sc</span><span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>Now, let’s build our classifier:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Perceptron</span>
<span class="n">perceptron</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">perceptron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="n">perceptron</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>To get a better intuition on the results we will plot the confusion matrix and decision boundary.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Wistia</span><span class="p">)</span>
<span class="n">classNames</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Negative'</span><span class="p">,</span><span class="s">'Positive'</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Perceptron Confusion Matrix - Entire Data'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True label'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Predicted label'</span><span class="p">)</span>
<span class="n">tick_marks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classNames</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classNames</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classNames</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="p">[[</span><span class="s">'TN'</span><span class="p">,</span><span class="s">'FP'</span><span class="p">],</span> <span class="p">[</span><span class="s">'FN'</span><span class="p">,</span> <span class="s">'TP'</span><span class="p">]]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span><span class="n">i</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span><span class="o">+</span><span class="s">" = "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="alignnone size-full wp-image-91" src="http://www.tarekatwan.com/wp-content/uploads/2017/12/fig6.png" alt="" width="301" height="307"></p>

<p>Now, let’s draw our decision boundary:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">X_set</span><span class="p">,</span> <span class="n">y_set</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span> <span class="o">=</span> <span class="n">X_set</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="n">X_set</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">),</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span> <span class="o">=</span> <span class="n">X_set</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="n">X_set</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">perceptron</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">X1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">X2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X1</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
             <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">((</span><span class="s">'navajowhite'</span><span class="p">,</span> <span class="s">'darkkhaki'</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">X1</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">X1</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">X2</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">X2</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_set</span><span class="p">)):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_set</span><span class="p">[</span><span class="n">y_set</span> <span class="o">==</span> <span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_set</span><span class="p">[</span><span class="n">y_set</span> <span class="o">==</span> <span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">c</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">((</span><span class="s">'red'</span><span class="p">,</span> <span class="s">'green'</span><span class="p">))(</span><span class="n">i</span><span class="p">),</span> <span class="n">label</span> <span class="o">=</span> <span class="n">j</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Perceptron Classifier (Decision boundary for Setosa vs the rest)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Petal Length'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Petal Width'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="alignnone size-full wp-image-92" src="http://www.tarekatwan.com/wp-content/uploads/2017/12/fig7.png" alt="" width="409" height="278"></p>

<p>We can see that our Perceptron did converge and was able to classify Setosa from Non-Setosa with perfect accuracy because indeed the data is linearly separable. This would not be the case if the data was not linearly separable. So, let’s try it on another class.</p>

<p>Outputs below are for Versicolor class:</p>

<p><img class="alignnone size-full wp-image-93" src="http://www.tarekatwan.com/wp-content/uploads/2017/12/fig8.png" alt="" width="310" height="307"> <img class="alignnone size-full wp-image-94" src="http://www.tarekatwan.com/wp-content/uploads/2017/12/fig9.png" alt="" width="418" height="278"></p>

<h3 id="support-vector-machines">
<a class="anchor" href="#support-vector-machines" aria-hidden="true"><span class="octicon octicon-link"></span></a>Support Vector Machines</h3>

<p>Now, let’s examine another approach using <strong>Support Vector Machines (SVM)</strong> with a <strong>linear kernel</strong>. In order to test for Linear Separability we will pick a hard-margin (for maximum distance as opposed to soft-margin) SVM with a linear kernel. Now, if the intent was to train a model our choices would be completely different. But, since we are testing for linear separability, we want a rigid test that would fail (or produce erroneous results if not converging) to help us better assess the data at hand.</p>

<p><img class="alignnone size-full wp-image-101" src="http://www.tarekatwan.com/wp-content/uploads/2017/12/svm.png" alt="" width="400" height="431">
<em>Image source Wikipedia: Maximum-margin hyperplane</em> <sup id="fnref:6"><a href="#fn:6" class="footnote">5</a></sup></p>

<p>Now, let’s code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="n">svm</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s">'linear'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">svm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">predicted</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Wistia</span><span class="p">)</span>
<span class="n">classNames</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Negative'</span><span class="p">,</span><span class="s">'Positive'</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'SVM Linear Kernel Confusion Matrix - Setosa'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True label'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Predicted label'</span><span class="p">)</span>
<span class="n">tick_marks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classNames</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classNames</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classNames</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="p">[[</span><span class="s">'TN'</span><span class="p">,</span><span class="s">'FP'</span><span class="p">],</span> <span class="p">[</span><span class="s">'FN'</span><span class="p">,</span> <span class="s">'TP'</span><span class="p">]]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span><span class="n">i</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span><span class="o">+</span><span class="s">" = "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]))</span>
</code></pre></div></div>

<p>Here are the plots for the confusion matrix and decision boundary:</p>

<p><img class="alignnone size-full wp-image-95" src="http://www.tarekatwan.com/wp-content/uploads/2017/12/fig10.png" alt="" width="323" height="307"> <img class="alignnone size-full wp-image-96" src="http://www.tarekatwan.com/wp-content/uploads/2017/12/fig11.png" alt="" width="421" height="278"></p>

<p>Perfect separartion/classification indicating a linear separability.</p>

<p>Now, let’s examin and rerun the test against Versicolor class and we get the plots below. Interesting enough, we don’t see a decision boundary and the confusion matrix indicates the classifier is not doing a good job at all.</p>

<p><img class="alignnone size-full wp-image-97" src="http://www.tarekatwan.com/wp-content/uploads/2017/12/fig12.png" alt="" width="332" height="307"> <img class="alignnone size-full wp-image-98" src="http://www.tarekatwan.com/wp-content/uploads/2017/12/fig13.png" alt="" width="440" height="278"></p>

<p>Now, for fun and to demonstrate how powerful SVMs can be let’s apply a non-linear kernel. In this case we will apply a Gaussian Radial Basis Function known as RBF Kernel. A slight change to the code above and we get completely different results:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># we are picking Versicolor to be 1 and all other classes will be 0
</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="n">svm</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'rbf'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">svm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">predicted</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Wistia</span><span class="p">)</span>
<span class="n">classNames</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Negative'</span><span class="p">,</span><span class="s">'Positive'</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'SVM RBF Confusion Matrix - Versicolor'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True label'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Predicted label'</span><span class="p">)</span>
<span class="n">tick_marks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classNames</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classNames</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classNames</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="p">[[</span><span class="s">'TN'</span><span class="p">,</span><span class="s">'FP'</span><span class="p">],</span> <span class="p">[</span><span class="s">'FN'</span><span class="p">,</span> <span class="s">'TP'</span><span class="p">]]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span><span class="n">i</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span><span class="o">+</span><span class="s">" = "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]))</span>
</code></pre></div></div>

<p><img class="alignnone size-full wp-image-99" src="http://www.tarekatwan.com/wp-content/uploads/2017/12/fig14.png" alt="" width="304" height="307"> <img class="alignnone size-full wp-image-100" src="http://www.tarekatwan.com/wp-content/uploads/2017/12/fig15.png" alt="" width="426" height="278"></p>

<p>Hope this helps.</p>

<p><strong>References</strong>:</p>

<div class="footnotes">
  <ol>
    <li id="fn:2">
      <p>The Linear Separability Problem: Some Testing Methods http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.6481&amp;rep=rep1&amp;type=pdf <a href="#fnref:2" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:3">
      <p>A Simple Algorithm for Linear Separability Test http://mllab.csa.iisc.ernet.in/downloads/Labtalk/talk30_01_08/lin_sep_test.pdf <a href="#fnref:3" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:4">
      <p>Convex Optimization, Linear Programming: http://www.stat.cmu.edu/~ryantibs/convexopt-F13/scribes/lec2.pdf <a href="#fnref:4" class="reversefootnote">↩</a> <a href="#fnref:4:1" class="reversefootnote">↩<sup>2</sup></a></p>
    </li>
    <li id="fn:5">
      <p>Test for Linear Separability with Linear Programming in R https://www.joyofdata.de/blog/testing-linear-separability-linear-programming-r-glpk/ <a href="#fnref:5" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:6">
      <p>Support Vector Machine https://en.wikipedia.org/wiki/Support_vector_machine <a href="#fnref:6" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/blog/markdown/2017/12/31/linear-separability.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Notes on Artificial Intelligence, Machine Learning &amp; Deep Learning, Python, Mobile &amp; Development.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/tatwan" title="tatwan"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/tarekatwan" title="tarekatwan"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
